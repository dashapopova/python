{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'асфальт мимо цемент избегать зевака аплодисменты обитатель спальный аррондисман'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# In this kernel I'll show you how easy it is to preprocess the text in Russian.\n",
    "\n",
    "# You need to install two libraries:\n",
    "# * nltk - to get russian stopwords\n",
    "# * pymystem3 - for lemmatization\n",
    "\n",
    "# download stopwords corpus, you need to run it once\n",
    "import nltk\n",
    "nltk.download(\"stopwords\")\n",
    "#--------#\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from pymystem3 import Mystem\n",
    "from string import punctuation\n",
    "\n",
    "#Create lemmatizer and stopwords list\n",
    "mystem = Mystem() \n",
    "russian_stopwords = stopwords.words(\"russian\")\n",
    "\n",
    "#Preprocess function\n",
    "def preprocess_text(text):\n",
    "    tokens = mystem.lemmatize(text.lower())\n",
    "    tokens = [token for token in tokens if token not in russian_stopwords\\\n",
    "              and token != \" \" \\\n",
    "              and token.strip() not in punctuation]\n",
    "    \n",
    "    text = \" \".join(tokens)\n",
    "    \n",
    "    return text\n",
    "\n",
    "#Examples    \n",
    "preprocess_text(\"Ну что сказать, я вижу кто-то наступил на грабли, Ты разочаровал меня, ты был натравлен.\")\n",
    "#> 'сказать видеть кто-то наступать грабли разочаровывать натравлять'\n",
    "\n",
    "preprocess_text(\"По асфальту мимо цемента, Избегая зевак под аплодисменты. Обитатели спальных аррондисманов\")\n",
    "#> 'асфальт мимо цемент избегать зевака аплодисменты обитатель спальный аррондисман'\n",
    "\n",
    "#Thats all :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\smart_open\\ssh.py:34: UserWarning: paramiko missing, opening SSH/SCP/SFTP paths will be disabled.  `pip install paramiko` to suppress\n",
      "  warnings.warn('paramiko missing, opening SSH/SCP/SFTP paths will be disabled.  `pip install paramiko` to suppress')\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import gensim\n",
    "import logging\n",
    "import nltk.data\n",
    "import pandas as pd\n",
    "import urllib.request\n",
    "from bs4 import BeautifulSoup\n",
    "from nltk.corpus import stopwords\n",
    "from gensim.models import word2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████| 1/1 [00:03<00:00,  3.19s/it]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "from pymystem3 import Mystem\n",
    "from string import punctuation\n",
    "\n",
    "m = Mystem()\n",
    "sw = stopwords.words(\"russian\")\n",
    "\n",
    "def preprocess_text(text):\n",
    "    tokens = m.lemmatize(text.lower())\n",
    "    tokens = [token for token in tokens if token not in sw \\\n",
    "              and token != \" \" \\\n",
    "              and token.strip() not in punctuation] \n",
    "    text = \" \".join(tokens)\n",
    "    \n",
    "    return text\n",
    "\n",
    "with open('C:\\!!_info_!!\\Desktop\\liza.txt', 'r', encoding='utf8') as f:\n",
    "    text1 = f.readlines()\n",
    "\n",
    "new_lines = []\n",
    "\n",
    "for line in tqdm(text1):\n",
    "    newline = preprocess_text(line)\n",
    "    #newline1 = re.sub(r'[^\\w\\s]','',newline)\n",
    "    new_lines.append(newline)\n",
    "        \n",
    "with open('C:\\!!_info_!!\\Desktop\\liza_lem.txt', 'w', encoding='utf8') as f1:\n",
    "    for line in new_lines:\n",
    "        f1.write(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = 'C:\\!!_info_!!\\Desktop\\liza_lem.txt'\n",
    "data = gensim.models.word2vec.LineSentence(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\gensim\\models\\base_any2vec.py:743: UserWarning: C extension not loaded, training will be slow. Install a C compiler and reinstall gensim for fast training.\n",
      "  \"C extension not loaded, training will be slow. \"\n",
      "2019-04-20 07:06:18,530 : INFO : collecting all words and their counts\n",
      "2019-04-20 07:06:18,534 : WARNING : this function is deprecated, use smart_open.open instead\n",
      "2019-04-20 07:06:18,546 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "2019-04-20 07:06:18,552 : INFO : collected 1201 word types from a corpus of 3564 raw words and 1 sentences\n",
      "2019-04-20 07:06:18,556 : INFO : Loading a fresh vocabulary\n",
      "2019-04-20 07:06:18,564 : INFO : effective_min_count=2 retains 474 unique words (39% of original 1201, drops 727)\n",
      "2019-04-20 07:06:18,569 : INFO : effective_min_count=2 leaves 2837 word corpus (79% of original 3564, drops 727)\n",
      "2019-04-20 07:06:18,575 : INFO : deleting the raw counts dictionary of 1201 items\n",
      "2019-04-20 07:06:18,578 : INFO : sample=0.001 downsamples 69 most-common words\n",
      "2019-04-20 07:06:18,580 : INFO : downsampling leaves estimated 1948 word corpus (68.7% of prior 2837)\n",
      "2019-04-20 07:06:18,586 : INFO : estimated required memory for 474 words and 300 dimensions: 1374600 bytes\n",
      "2019-04-20 07:06:18,588 : INFO : resetting layer weights\n",
      "2019-04-20 07:06:18,618 : INFO : training model with 3 workers on 474 vocabulary and 300 features, using sg=0 hs=0 sample=0.001 negative=5 window=5\n",
      "2019-04-20 07:06:18,638 : WARNING : this function is deprecated, use smart_open.open instead\n",
      "2019-04-20 07:06:18,666 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-04-20 07:06:18,670 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-04-20 07:06:19,810 : INFO : EPOCH 1 - PROGRESS: at 100.00% examples, 1701 words/s, in_qsize 0, out_qsize 1\n",
      "2019-04-20 07:06:19,815 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-04-20 07:06:19,817 : INFO : EPOCH - 1 : training on 3564 raw words (1961 effective words) took 1.2s, 1689 effective words/s\n",
      "2019-04-20 07:06:19,830 : WARNING : this function is deprecated, use smart_open.open instead\n",
      "2019-04-20 07:06:19,858 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-04-20 07:06:19,864 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-04-20 07:06:20,931 : INFO : EPOCH 2 - PROGRESS: at 100.00% examples, 1802 words/s, in_qsize 0, out_qsize 1\n",
      "2019-04-20 07:06:20,935 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-04-20 07:06:20,937 : INFO : EPOCH - 2 : training on 3564 raw words (1951 effective words) took 1.1s, 1791 effective words/s\n",
      "2019-04-20 07:06:20,947 : WARNING : this function is deprecated, use smart_open.open instead\n",
      "2019-04-20 07:06:20,984 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-04-20 07:06:20,997 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-04-20 07:06:22,017 : INFO : EPOCH 3 - PROGRESS: at 100.00% examples, 1869 words/s, in_qsize 0, out_qsize 1\n",
      "2019-04-20 07:06:22,022 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-04-20 07:06:22,024 : INFO : EPOCH - 3 : training on 3564 raw words (1946 effective words) took 1.0s, 1856 effective words/s\n",
      "2019-04-20 07:06:22,037 : WARNING : this function is deprecated, use smart_open.open instead\n",
      "2019-04-20 07:06:22,071 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-04-20 07:06:22,077 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-04-20 07:06:23,064 : INFO : EPOCH 4 - PROGRESS: at 100.00% examples, 1943 words/s, in_qsize 0, out_qsize 1\n",
      "2019-04-20 07:06:23,069 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-04-20 07:06:23,072 : INFO : EPOCH - 4 : training on 3564 raw words (1951 effective words) took 1.0s, 1928 effective words/s\n",
      "2019-04-20 07:06:23,083 : WARNING : this function is deprecated, use smart_open.open instead\n",
      "2019-04-20 07:06:23,117 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-04-20 07:06:23,130 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-04-20 07:06:24,257 : INFO : EPOCH 5 - PROGRESS: at 100.00% examples, 1710 words/s, in_qsize 0, out_qsize 1\n",
      "2019-04-20 07:06:24,262 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-04-20 07:06:24,264 : INFO : EPOCH - 5 : training on 3564 raw words (1965 effective words) took 1.2s, 1700 effective words/s\n",
      "2019-04-20 07:06:24,267 : INFO : training on a 17820 raw words (9774 effective words) took 5.6s, 1732 effective words/s\n",
      "2019-04-20 07:06:24,274 : WARNING : under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 5.75 s\n"
     ]
    }
   ],
   "source": [
    "%time model_liza = gensim.models.Word2Vec(data, size=300, window=5, min_count=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-04-20 07:06:24,303 : INFO : precomputing L2-norms of word weight vectors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-04-20 07:06:24,333 : INFO : storing 474x300 projection weights into liza.bin\n",
      "2019-04-20 07:06:24,341 : WARNING : this function is deprecated, use smart_open.open instead\n"
     ]
    }
   ],
   "source": [
    "model_liza.init_sims(replace=True)\n",
    "model_path = \"liza.bin\"\n",
    "\n",
    "print(\"Saving model...\")\n",
    "model_liza.wv.save_word2vec_format(model_path, binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "474\n"
     ]
    }
   ],
   "source": [
    "print(len(model_liza.wv.vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['!..', '),', ',', '...', ':', '?..', '«', '»', '»,', 'ангел', 'анюта', 'армия', 'ах', 'барин', 'бедный', 'белый', 'берег', 'березовый', 'беречь', 'бесчисленный', 'благодарить', 'бледный', 'блеснуть', 'блестящий', 'близ', 'бог', 'богатый', 'большой', 'бояться', 'брать', 'бросать', 'бросаться', 'бывать', 'важный', 'ввечеру', 'вдова', 'велеть', 'великий', 'великолепный', 'верить', 'верно', 'весело', 'веселый', 'весна', 'вести', 'весь', 'весьма', 'ветвь', 'ветер', 'вечер', 'взглядывать', 'вздох', 'вздыхать', 'взор', 'взять', 'вид', 'видеть', 'видеться', 'видный', 'вместе', 'вода', 'возвращаться', 'воздух', 'война', 'воображать', 'воображение', 'воспоминание', 'восторг', 'восхищаться', 'время', 'вслед', 'вставать', 'встречаться', 'всякий', 'высокий', 'выть', 'выходить', 'глаз', 'глубокий', 'гнать', 'говорить', 'год', 'голос', 'гора', 'горе', 'горестный', 'горлица', 'город', 'горький', 'господин', 'гром', 'грусть', 'давать', 'давно', 'далее', 'дверь', 'движение', 'двор', 'девушка', 'дело', 'день', 'деньги', 'деревня', 'деревянный', 'десять', 'добро', 'добрый', 'довольно', 'доживать', 'долго', 'должный', 'дом', 'домой', 'дочь', 'древний', 'друг', 'дуб', 'думать', 'душа', 'едва', 'ехать', 'жалобный', 'желание', 'желать', 'жениться', 'жених', 'женщина', 'жестокий', 'живой', 'жизнь', 'жить', 'забава', 'заблуждение', 'забывать', 'завтра', 'задумчивость', 'закраснеться', 'закричать', 'заря', 'здешний', 'здравствовать', 'зеленый', 'земля', 'златой', 'знать', 'ибо', 'играть', 'идти', 'имя', 'искать', 'исполняться', 'испугаться', 'история', 'исчезать', 'кабинет', 'казаться', 'капля', 'карета', 'карман', 'картина', 'катиться', 'клятва', 'колено', 'копейка', 'который', 'красота', 'крест', 'крестьянин', 'крестьянка', 'кровь', 'кроме', 'купить', 'ландыш', 'ласка', 'ласковый', 'левый', 'лес', 'лететь', 'летний', 'лето', 'лиза', 'лизин', 'лизина', 'лицо', 'лишний', 'лодка', 'ложиться', 'луг', 'луч', 'любезный', 'любить', 'любовь', 'лютый', 'матушка', 'мать', 'место', 'месяц', 'мечта', 'милый', 'мимо', 'минута', 'многочисленный', 'молить', 'молиться', 'молния', 'молодой', 'молодость', 'молчать', 'монастырь', 'море', 'москва', 'москва-река', 'мочь', 'мрак', 'мрачный', 'муж', 'мысль', 'наглядеться', 'надеяться', 'надлежать', 'надобно', 'называть', 'наступать', 'натура', 'находить', 'наш', 'небесный', 'небо', 'невинность', 'невинный', 'нежели', 'нежный', 'незнакомец', 'немой', 'непорочность', 'неприятель', 'несколько', 'никакой', 'никто', 'ничто', 'новый', 'ночь', 'обижать', 'облако', 'обманывать', 'обморок', 'образ', 'обращаться', 'обстоятельство', 'объятие', 'огонь', 'однако', 'окно', 'окрестности', 'оно', 'опираться', 'описывать', 'опустеть', 'освещать', 'оставаться', 'оставлять', 'останавливать', 'останавливаться', 'отвечать', 'отдавать', 'отец', 'отечество', 'отменно', 'отрада', 'очень', 'падать', 'память', 'пастух', 'первый', 'перемениться', 'переставать', 'песня', 'петь', 'печальный', 'писать', 'питать', 'плакать', 'побежать', 'побледнеть', 'погибать', 'подавать', 'подгорюниваться', 'подле', 'подозревать', 'подымать', 'поехать', 'пойти', 'показываться', 'поклониться', 'покойный', 'покрывать', 'покрываться', 'покупать', 'полагать', 'поле', 'помнить', 'поселянин', 'последний', 'постой', 'потуплять', 'поцеловать', 'поцелуй', 'правый', 'представляться', 'прежде', 'преклонять', 'прекрасный', 'прелестный', 'приводить', 'прижимать', 'принадлежать', 'принуждать', 'природа', 'приходить', 'приятный', 'провожать', 'продавать', 'проливать', 'простой', 'просыпаться', 'проходить', 'проч', 'прощать', 'прощаться', 'пруд', 'птичка', 'пылать', 'пять', 'работа', 'работать', 'радость', 'рассказывать', 'расставаться', 'рвать', 'ребенок', 'река', 'решаться', 'робкий', 'роза', 'розовый', 'роман', 'российский', 'роща', 'рубль', 'рука', 'сажень', 'самый', 'свет', 'светиться', 'светлый', 'свидание', 'свирель', 'свободно', 'свой', 'свойство', 'сделать', 'сделаться', 'сей', 'сердечный', 'сердце', 'си', 'сидеть', 'сие', 'сиять', 'сказать', 'сказывать', 'сквозь', 'скорбь', 'скоро', 'скрываться', 'слабый', 'слеза', 'слезать', 'слово', 'случаться', 'слушать', 'слышать', 'смерть', 'сметь', 'смотреть', 'собственный', 'соглашаться', 'солнце', 'спасать', 'спать', 'спокойно', 'спокойствие', 'спрашивать', 'стадо', 'становиться', 'стараться', 'старуха', 'старушка', 'старый', 'статься', 'стена', 'сто', 'столь', 'стон', 'стонать', 'сторона', 'стоять', 'страшно', 'страшный', 'судьба', 'схватывать', 'счастие', 'счастливый', 'сын', 'таить', 'твой', 'темный', 'тения', 'тихий', 'тихонько', 'томный', 'трава', 'трепетать', 'трогать', 'убивать', 'уверять', 'увидеть', 'увидеться', 'удерживать', 'удивляться', 'удовольствие', 'узнавать', 'улица', 'улыбка', 'уметь', 'умирать', 'унылый', 'упасть', 'услышать', 'утешение', 'утро', 'хижина', 'хлеб', 'ходить', 'холм', 'хороший', 'хотеть', 'хотеться', 'хотя', 'худо', 'худой', 'царь', 'цветок', 'целовать', 'час', 'часто', 'человек', 'чистый', 'читатель', 'чувствительный', 'чувство', 'чувствовать', 'чудно', 'чулок', 'шестой', 'шум', 'шуметь', 'щадить', 'щека', 'эраст', 'эрастов', 'это', '—', '„']\n"
     ]
    }
   ],
   "source": [
    "print(sorted([w for w in model_liza.wv.vocab]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('сей', 0.43026119470596313)]\n",
      "[('—', 0.560495138168335), ('свой', 0.5543678998947144), ('мочь', 0.5441586971282959)]\n",
      "0.7466533\n",
      "грусть\n",
      "['—', '«', '»', 'свой', 'мочь', 'который']\n"
     ]
    }
   ],
   "source": [
    "print(model_liza.wv.most_similar(positive=[\"смерть\", \"любовь\"], negative=[\"печальный\"], topn=1))\n",
    "\n",
    "print(model_liza.wv.most_similar(\"любовь\", topn=3))\n",
    "\n",
    "print(model_liza.wv.similarity(\"лиза\", \"эраст\"))\n",
    "\n",
    "print(model_liza.wv.doesnt_match(\"скорбь грусть слеза улыбка\".split()))\n",
    "\n",
    "print(model_liza.wv.words_closer_than(\"лиза\", \"эраст\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
